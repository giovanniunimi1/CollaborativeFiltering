{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1m9HYhrgsbz926fIehXRh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giovanniunimi1/CollaborativeFiltering/blob/main/CFIB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZRdsPGXp4Gb",
        "outputId": "4b21a78d-7500-4026-d975-8eaf9979f0bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "#DOWNLOAD SPARK AND SPARK DEPENDENCIES (JDK AND FINDSPARK)\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "\n",
        "! pip install -q kaggle\n",
        "#set kaggle for dataset (remove # for setting kaggle api for downloading dataset)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "os.environ['KAGGLE_USERNAME'] = \"giovannibuscemi\"\n",
        "os.environ['KAGGLE_KEY'] = \"XXXXXXXX\"\n",
        "!kaggle datasets download -d yelp-dataset/yelp-dataset\n",
        "! unzip yelp-dataset.zip -d yelp-dataset\n",
        "\n",
        "#NGROCK FOR MONITORING PYSPARK EXECUTION\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "!pip install pyngrok\n",
        "import getpass\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import findspark\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").config('spark.ui.port', '4050').appName(\"CFIB\").getOrCreate()\n",
        "\n",
        "print(\"Enter your authtoken, can be copied \"\n",
        "\"from https://dashboard.ngrok.com\")\n",
        "conf.get_default().auth_token = getpass.getpass()\n",
        "\n",
        "ui_port = 4050\n",
        "public_url= ngrok.connect(ui_port).public_url\n",
        "print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{ui_port}\\\"\")\n",
        "\n",
        "#UTIL FOR COMPUTING\n",
        "def create_dict(values):\n",
        "    dict = {}\n",
        "    for key,value in values:\n",
        "        dict.update({key:value})\n",
        "    return dict\n",
        "def normalize_row(values):\n",
        "    row_values = list(values)\n",
        "    mean = sum(value[1] for value in row_values) / len(row_values)\n",
        "    updated_values = [(value[0], value[1] - mean) for value in row_values]\n",
        "    return updated_values\n",
        "#USED TO MAP NORMALIZED UTILITY MATRIX IN A COLUMN REPRESENTATION FOR SIMILARITY COMPUTING\n",
        "def transform_row(row):\n",
        "    user, business_ratings = row\n",
        "    return [(business, (user, rating)) for business, rating in business_ratings]\n",
        "#########################################\n",
        "############# MAIN FUNCTION #############\n",
        "#########################################\n",
        "\n",
        "#Create Utility Matrix in sparse form\n",
        "def map_to_indices(row,user_map,business_map):\n",
        "        user_id = row.user_id\n",
        "        business_id = row.business_id\n",
        "        user_index = user_map.get(user_id, -1)\n",
        "        business_index = business_map.get(business_id, -1)\n",
        "        if user_index == -1 or business_index == -1:\n",
        "            pass\n",
        "        else :\n",
        "            return (user_index, (business_index, row.stars))\n",
        "\n",
        "def calculate_similarity(pair,columns_broadcast):\n",
        "    columns = columns_broadcast.value\n",
        "    business1, business2 = pair\n",
        "    if business1==business2:\n",
        "        return None\n",
        "    dict1 = columns[business1]\n",
        "    dict2 = columns[business2]\n",
        "    common = set(dict1.keys()) & set(dict2.keys())\n",
        "    if not common:\n",
        "        return None\n",
        "    else :\n",
        "        numerator = 0\n",
        "        denominator1 = 0\n",
        "        denominator2 = 0\n",
        "        for key in common:\n",
        "            numerator += dict1[key] * dict2[key]\n",
        "            denominator1 += dict1[key] ** 2\n",
        "            denominator2 += dict2[key] ** 2\n",
        "        if denominator1 == 0 or denominator2 == 0:\n",
        "            return None\n",
        "        else:\n",
        "            similarity = numerator / (denominator1 ** 0.5 * denominator2 ** 0.5)\n",
        "            return (business1,(business2,similarity) )\n",
        "\n",
        "#Create Blank Prediction Matrix\n",
        "def upgrade_prediction(row,global_index,similarity_rdd,k):\n",
        "    similarity_matrix = similarity_rdd.value\n",
        "    result = []\n",
        "\n",
        "    ind = set(global_index) - set(row.keys())\n",
        "    avg = sum(row.values())/len(row)\n",
        "    for i in ind:\n",
        "            if i in similarity_matrix:\n",
        "                similarity_dict = similarity_matrix[i]\n",
        "                #TOP_K_SIMILARITIES :\n",
        "                sorted_dict = sorted(similarity_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "                top_k = dict(sorted_dict[:k])\n",
        "                common = set(top_k.keys()) & set(row.keys())\n",
        "                numerator=0\n",
        "                denominator=0\n",
        "                for key in common:\n",
        "                    numerator += row[key]*top_k[key]\n",
        "                    denominator += top_k[key]\n",
        "                score = avg + (numerator / denominator) if denominator != 0 and numerator != 0 else -5\n",
        "                if score != -5:\n",
        "                    result.append({i: score})\n",
        "            else :\n",
        "                print('negative result')\n",
        "\n",
        "\n",
        "    return sorted(result, key=lambda x: list(x.values())[0], reverse=True)\n",
        "\n",
        "\n",
        "\n",
        "#READ DATAFRAME AND CREATE DICTS FORM USER AND BUSINESS MAP :\n",
        "df_review = spark.read.json(\"yelp-dataset/yelp_academic_dataset_review.json\")\n",
        "#PREPROCESS :\n",
        "df_u = spark.read.json(\"yelp-dataset/yelp_academic_dataset_user.json\")\n",
        "df_b = spark.read.json(\"yelp-dataset/yelp_academic_dataset_business.json\")\n",
        "#GLOBAL VARIABLE-HYPERPARAMETER\n",
        "k = 10\n",
        "t = 100\n",
        "frac = 0.001\n",
        "\n",
        "starting_user = df_u.filter(df_u[\"review_count\"]>500).select(\"user_id\")      #.withColumnRenamed(\"user_id\")\n",
        "starting_users = starting_user.sample(fraction)\n",
        "\n",
        "\n",
        "business = df_review.join(starting_users,df_review.user_id == starting_users.user_id,\"inner\")\\\n",
        "            .select(\"business_id\").distinct().withColumnRenamed(\"business_id\",\"business_id1\")\n",
        "users = df_review .join(business,df_review.business_id == business.business_id1,\"inner\")\\\n",
        "          .select(\"user_id\").distinct().withColumnRenamed(\"user_id\",\"user_id1\")\n",
        "df_filtered = df_review.join(business,df_review.business_id == business.business_id1,\"inner\")\\\n",
        "                .join(users,df_review.user_id == users.user_id1,\"inner\")\\\n",
        "                .select(\"user_id\",\"business_id\",\"stars\")\n",
        "#CREATE MAP FOR ID TO INTEGER\n",
        "unique_user_ids = users.select('user_id1').rdd.map(lambda row: row[0]).collect()\n",
        "unique_business_ids = business.select('business_id1').rdd.map(lambda row: row[0]).collect()\n",
        "\n",
        "user_map = {user_id: idx for idx, user_id in enumerate(unique_user_ids)}\n",
        "business_map = {business_id: idx for idx, business_id in enumerate(unique_business_ids)}\n",
        "\n",
        "#UTILITY MATRIX BY BUSINESS :\n",
        "utility_row= df_filtered.rdd.map(lambda row: map_to_indices(row, user_map, business_map)) \\\n",
        "                        .filter(lambda pair: pair[0] != -1 and pair[1][0] != -1)\\\n",
        "                        .groupByKey().mapValues(lambda values : normalize_row(values))\n",
        "\n",
        "columns = utility_row.flatMap(transform_row).groupByKey().mapValues(lambda values : create_dict(values))\n",
        "\n",
        "#convert tuple into dict to optimize the operation\n",
        "utility_row = utility_row.mapValues(lambda values : create_dict(values))\n",
        "\n",
        "business_combinations = columns.keys() \\\n",
        "    .cartesian(columns.keys()) \\\n",
        "#Broadcast of Utility matrix by column\n",
        "columns_broadcast = spark.sparkContext.broadcast(columns.collectAsMap())\n",
        "#similarity matrix :\n",
        "similarities = business_combinations.map(lambda pair: calculate_similarity(pair, columns_broadcast)).filter(lambda x: x is not None)\n",
        "similarities_row = similarities.groupByKey().mapValues(lambda values : create_dict(values))\n",
        "\n",
        "#Broadcast of similarity matrix for prediction computing\n",
        "similarity_rdd = spark.sparkContext.broadcast(similarities_row.collectAsMap())\n",
        "#index for blank value computing\n",
        "global_index =  business_map.values()\n",
        "\n",
        "upgraded_matrix = utility_row.map(lambda row: (row[0],upgrade_prediction(row[1],global_index,similarity_rdd,k)))\n",
        "upgraded_matrix.saveAsTextFile(\"output\")"
      ]
    }
  ]
}